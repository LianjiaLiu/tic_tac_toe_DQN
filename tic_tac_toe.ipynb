{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise: Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T18:04:27.094062Z",
     "start_time": "2024-01-07T18:04:26.705545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "\n",
    "from typing import Any, Dict, Callable\n",
    "from dataclasses import dataclass\n",
    "from functools import cached_property, partial\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Below is the introduction of RL team***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "The task of the programming exercise is to program an agent that learns to play Tic-Tac-Toe against different opponents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opponent Policies\n",
    "\n",
    "We load and initialize the opponent policies of increasing strength from .json-files. They are stored in global variables and can therefore be easily changed at any point of the notebook. The opponent policies are of increasing strength.\n",
    "\n",
    "For loading the policies, the folder `Opponent_Policies` containing the .json-files of the policies should lie in the same directory as this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T18:04:29.306066Z",
     "start_time": "2024-01-07T18:04:29.147798Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright by Reinforcement Learning 23/24 team of Saarland University\n",
    "\"\"\"\n",
    "\n",
    "# Load opponent policy from .json-file. \n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "opponent_policy_file = Path('Opponent_Policies') # Change filename to play against different policy.\n",
    "\n",
    "with open(opponent_policy_file / 'policy1.json') as json_file:\n",
    "    opponent_policy_1 = json.load(json_file)\n",
    "\n",
    "with open(opponent_policy_file / 'policy2.json') as json_file:\n",
    "    opponent_policy_2 = json.load(json_file)\n",
    "\n",
    "with open(opponent_policy_file / 'policy3.json') as json_file:\n",
    "    opponent_policy_3 = json.load(json_file)\n",
    "\n",
    "with open(opponent_policy_file / 'policy4.json') as json_file:\n",
    "    opponent_policy_4 = json.load(json_file)\n",
    "\n",
    "# Set opponent policy\n",
    "opponent_policy_dict = opponent_policy_1 # Change to play against different opponent policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Environment for Tic-Tac-Toe\n",
    "\n",
    "We implement a Gymnasium environment simulating a game of Tic-Tac-Toe. We thereby use\n",
    "- as possible field values  $V = \\{0,1,2\\}$, whereby $v = 0$ stands for a 'O'-field, $v = 1$ for an empty field, and $v = 2$ for a 'X'-field.\n",
    "- as state space $S = V^{3 \\times 3}$. A state `s` is stored as a `list[list[int]]`, `s[i][j]` refers then to the value in the i-th row in the j-th column.\n",
    "- action space $A = V \\times V = \\{(0,0),(0,1),(0,2),(1,0),(1,1),(1,2), (2,0),(2,1),(2,2)\\}$.\n",
    "\n",
    "### Environment Dynamics\n",
    "\n",
    "We implement Tic-Tac-Toe as a sequential decision problem. The agent plays against a specified opponent policy (see above). One step of the environment looks as follows: \n",
    "1. Perform the move of the agent. The agent marks fields with 'X'.\n",
    "2. Check whether this has finished the game, i.e. win for the agent or draw. If the game is finished, terminate episode and compute the reward. \n",
    "3. Perform the game of the opponent. The opponent marks fields with 'O'.\n",
    "4. Check whether this has finished the game, i.e. win for the opponent or draw. If the game is finished, terminate episode and compute the reward. \n",
    "\n",
    "#### Initial state\n",
    "We randomize whether the agent or the opponent starts with the first move. Hence, the initial state of the sequential decision problem is either\n",
    "- a completely empty field, for the case that the agent has the first move, or\n",
    "- a field with one '0', for the case that the opponent has the first move. \n",
    "\n",
    "\n",
    "#### Rewards\n",
    "Rewards are only gained when the game is finished: \n",
    "- Reward of 1, if the agent wins. \n",
    "- Reward of 0, if the game ends in a draw. \n",
    "- Reward of -1, if the opponent wins. \n",
    "\n",
    "#### Executable Actions \n",
    "Notice that not all actions are always executable: If a field `s[i][j]` is non-empty, then the action $(i,j)$ is not executable. If the agent tries to perform a non-executable action, the environment raises an Exception. Hence, make sure that the agent only picks executable actions (the opponent policy chooses only executable actions as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T18:04:30.769667Z",
     "start_time": "2024-01-07T18:04:30.741468Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright by Reinforcement Learning 23/24 team of Saarland University\n",
    "\"\"\"\n",
    "\n",
    "# Some preliminary and auxiliary definitions \n",
    "\n",
    "# Definitions of possible field values\n",
    "CROSS, EMPTY, CIRCLE = 2, 1, 0  \n",
    "\n",
    "def get_rows(state: list[list[int]]) -> [list[list[int]], list[list[int]], list[list[int]]]:\n",
    "    \"\"\"\n",
    "    Helper function: Returns list of rows, list of columns, and list of diagonals\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Compute rows\n",
    "    rows = state\n",
    "\n",
    "    # Compute columns\n",
    "    columns = []\n",
    "    for j in range(3):\n",
    "        column = []\n",
    "        for i in range(3):\n",
    "            column.append(state[i][j])\n",
    "        columns.append(column)\n",
    "    \n",
    "    #Compute diagonals\n",
    "    diagonal0 = []\n",
    "    diagonal1 = []\n",
    "    for i in range(3):\n",
    "        diagonal0.append(state[i][i])\n",
    "        diagonal1.append(state[2-i][i])\n",
    "    \n",
    "    # Return rows, columns, and diagonals. \n",
    "    return rows, columns, [diagonal0, diagonal1]\n",
    "\n",
    "# Gymnasium environment for Tic-Tac-Toe\n",
    "class SysadminEnv(gym.Env):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.action_space = spaces.MultiDiscrete([3,3]) # Action space \n",
    "        self.observation_space = spaces.MultiDiscrete([[3,3,3],[3,3,3],[3,3,3]]) # State space\n",
    "        self.reset_counter = 0\n",
    "\n",
    "\n",
    "    @property\n",
    "    def get_reset_counter(self):\n",
    "        return self.reset_counter\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def occupied_fields(self) -> int | None:\n",
    "        \"\"\"\n",
    "        Returns the number of occupied fields.\n",
    "\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"_state\"):\n",
    "            return None\n",
    "        \n",
    "        res = 0\n",
    "        for l in self._state:\n",
    "            for v in l:\n",
    "                if v != EMPTY:\n",
    "                    res = res + 1\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def game_finished(self) -> int | None:\n",
    "        \"\"\"\n",
    "        Returns None if game is not finished.\n",
    "\n",
    "        Returns 0 if circle wins.\n",
    "        Returns 1 if it is a draw.\n",
    "        Returns 2 if crosses wins. \n",
    "\n",
    "        \"\"\"\n",
    "        rows, columns, diagonals = get_rows(self._state)\n",
    "\n",
    "        for l in rows + columns + diagonals:\n",
    "            if all(v == CROSS for v in l):\n",
    "                return 2\n",
    "            if all(v == CIRCLE for v in l):\n",
    "                return 0\n",
    "            \n",
    "        if self.occupied_fields == 9:\n",
    "            return 1\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def opponent_policy (self) -> [int,int]:\n",
    "        \"\"\"\n",
    "            Takes random action from the list of moves of the opponent policy.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"_state\"):\n",
    "            raise Exception(\"Unable to find opponent move in uninitialized environment.\")\n",
    "        \n",
    "        opponent_action_list = opponent_policy_dict[self._state.__str__()]\n",
    "        return opponent_action_list[np.random.choice(len(opponent_action_list))]\n",
    "\n",
    "    \n",
    "    def perform_move(self, move: [int, int], cross: bool):\n",
    "        \"\"\"\n",
    "        Returns the number of occupied fields.\n",
    "\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"_state\"):\n",
    "            raise Exception(\"Unable to perform move in uninitialized environment.\")\n",
    "#         print(move)\n",
    "        if self._state[move[0]][move[1]] != EMPTY:\n",
    "            raise Exception(\"Unable to perform move on occupied field.\")\n",
    "\n",
    "        if cross: \n",
    "            self._state[move[0]][move[1]] = 2\n",
    "        else:\n",
    "            self._state[move[0]][move[1]] = 0\n",
    "\n",
    "        \n",
    "    def reset(\n",
    "        self, *, seed: int | None = None, options: dict[str, Any] | None = None\n",
    "    ) -> tuple[np.ndarray, dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Resets the environment to its initial state.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # increment reset_counter\n",
    "        self.reset_counter += 1\n",
    "\n",
    "        # All fields are empty initially\n",
    "        self._state = [[EMPTY,EMPTY,EMPTY],[EMPTY,EMPTY,EMPTY],[EMPTY,EMPTY,EMPTY]]\n",
    "\n",
    "        # Random choice whether agent or opponent makes the first move. \n",
    "        # In case of opponent, first move of opponent is performed.\n",
    "        if np.random.random() < 0.5:   \n",
    "            self.perform_move(self.opponent_policy(), False)\n",
    "        \n",
    "        return self._state, dict()\n",
    "   \n",
    "\n",
    "    def step(self, action: [int,int]) -> tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Performs a step in the environment given an action of the agent.\n",
    "\n",
    "        Return: new_state, reward, done, truncated, information_dictionary (last two return values are irrelevant for our purposes)  \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform agent's move\n",
    "        self.perform_move(action, True)\n",
    "\n",
    "        # Check whether game is finished and compute the return\n",
    "        finished = self.game_finished  \n",
    "        if self.game_finished == 2:\n",
    "            return self._state, 1, True, False, dict()\n",
    "        if self.game_finished == 1:\n",
    "            return self._state, 0, True, False, dict()\n",
    "\n",
    "\n",
    "        # Perform opponent's move\n",
    "        self.perform_move(self.opponent_policy(), False)\n",
    "\n",
    "        # Check whether game is finished and compute the return\n",
    "        finished = self.game_finished  \n",
    "        if finished is None:\n",
    "            return self._state, 0, False, False, dict()\n",
    "        elif finished == 0:\n",
    "            return self._state, -1, True, False, dict()\n",
    "        elif finished == 1:\n",
    "            return self._state, 0, True, False, dict()    \n",
    "\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"\n",
    "        Prints the current state of the field to the command line. \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"_state\"):\n",
    "           raise Exception(\"Unable to visualize uninitialized environment.\")\n",
    "       \n",
    "        res = [[\"\",\"\",\"\"],[\"\",\"\",\"\"],[\"\",\"\",\"\"]] \n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                v = self._state[i][j]\n",
    "                if v == CROSS:\n",
    "                    res[i][j] = \"X\"\n",
    "                elif v == EMPTY:\n",
    "                    res[i][j] = \" \"\n",
    "                elif v == CIRCLE:\n",
    "                    res[i][j] = \"O\"\n",
    "                else: \n",
    "                    raise Exception(\"Invalid value in TicTacToe Field\")\n",
    "\n",
    "        for l in res: \n",
    "            print(l)\n",
    "    \n",
    "        print(\"\\n\")\n",
    "\n",
    "# Register environment\n",
    "gym.register(\"Sysadmin-ED\", partial(SysadminEnv))\n",
    "env = gym.make(\"Sysadmin-ED\")\n",
    "\n",
    "# Example on how to experiment with the environment. \n",
    "\n",
    "env.reset(seed=42)\n",
    "\n",
    "print(env.step((1,2)))\n",
    "\n",
    "env.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Above is the instruction provided by the RL 23/24 team***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (Description) \n",
    "\n",
    "Briefly describe the learning algorithm you have implemented. When using function approximation, describe in particular the features that you are using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameterized function approximators are used to represent the action-value function. For the approximators, instead of the linear ones, we choose to implement deep neural networks (DNNs) to avoid handcrafting a suitable set of features.  \n",
    "\n",
    "The architecture is shown as below.   \n",
    "\n",
    "```python\n",
    "DQN(  \n",
    "  (layer1): Linear(in_features=9, out_features=32, bias=True)  \n",
    "  (layer2): Linear(in_features=32, out_features=32, bias=True)  \n",
    "  (layer3): Linear(in_features=32, out_features=9, bias=True)  \n",
    ")  \n",
    "```\n",
    "The number of parameters:  1673\n",
    "\n",
    "The input is a 9-dimensional vector, which is the flattened version of the 3x3 board. The output is a 9-dimensional vector, which is the Q-values of the 9 actions. To avoid the problem of predicting the Q-values of non-executable actions, we mask them by setting their Q-values to -inf (during action and while training). The value with the highest Q-value is chosen as the action with probability of 1-epsilon, otherwise a random action is chosen. \n",
    "\n",
    "### Training\n",
    "Deep learning often requires large amounts of data -- thus we needed to implement bootstrapping to generate more data. We use the following bootstrapping strategy:\n",
    "- We start with a random policy and play 1000 episodes.\n",
    "- We then use the trained policy to play 1000 episodes and store the transitions in a replay buffer.\n",
    "- We use batch gradient descent to train the network on the replay buffer, which speeds up the training process.\n",
    "\n",
    "We use two networks -- the policy and target. The policy network is updated using the Adam optimizer based on the Q-value errors. The target network is updated as a running average of the policy network. \n",
    "\n",
    "The policy network's target in the loss is initialized as 0, then all the invalid states are masked as -inf, and the reward + the best action value (generated usign the target network) from the next state is chosen as the target Q-value. The loss is then computed as the `SmoothL1Loss` between the target and the predicted Q-values.\n",
    "\n",
    "The epsilon-greedy policy is used to balance exploration and exploitation. The epsilon is exponentially annealed from 0.8 to 0.05 during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (Implementation of DQN)\n",
    "\n",
    "Implement a learning algorithm that learns to play Tic-Tac-Toe. For function approximation learning, the function `RL_algorithm` should return the learned feature weights.\n",
    "\n",
    "In the case that you are not using function approximation but a different RL algorithm, the arguments and return types of the functions below are allowed to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T03:48:05.240403Z",
     "start_time": "2024-01-08T03:48:05.187787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Given hyperparameter gamma\n",
    "gamma = 1\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from itertools import count\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'action_mask'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "    def get_parameter_number(self):\n",
    "        total_num = sum(p.numel() for p in self.parameters())\n",
    "        trainable_num = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return {'Total': total_num, 'Trainable': trainable_num}\n",
    "    \n",
    "# Hyperparameters for training\n",
    "BATCH_SIZE = 80  # for mini-batch gradient descent\n",
    "EPS_START = 0.8  # e-greedy threshold start value\n",
    "EPS_END = 0.05   # e-greedy threshold end value\n",
    "EPS_DECAY = 1000 # e-greedy threshold decay, controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 0.01       # for soft update of parameters for the target net \n",
    "LR = 1e-4        # learning rate\n",
    "\n",
    "\n",
    "n_actions = 9 # The number of actions\n",
    "\n",
    "n_observations = 9 # The number of state observations\n",
    "\n",
    "def agent_policy(state):\n",
    "    \"\"\"\n",
    "    Policy of the agent: Given the environment state and feature weights, returns the best estimated performable action. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    state = torch.tensor(state, device=device, dtype=torch.float).flatten()\n",
    "    available_actions_mask = available_actions(state)\n",
    "                \n",
    "    with torch.no_grad():\n",
    "        action_index = policy_net(state)\n",
    "        action_index = torch.tensor([action_index[i] if available_actions_mask[i]!=0. else -float(\"Inf\") for i in range(n_actions) ], device = device, dtype = torch.float)\n",
    "        action_index = torch.argmax(action_index).item()\n",
    "        return (np.floor(action_index / 3).astype(int), int(action_index% 3) )\n",
    "    \n",
    "def training_algorithm(num_episodes: int, env: gym.Env, training_episode):\n",
    "    \"\"\"\n",
    "    Reinforcement learning algorithm: For function approximation learning, learn the feature weights for the given number of training episodes, i.e. env.reset() is allowed to be called num_episodes many times. \n",
    "    \n",
    "    Inputs: \n",
    "    num_episodes: number of training episodes\n",
    "    env: gymnasium environment\n",
    "    training_episode: the start point for updating the networks\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # counts how many times we select the actions, controls the update of the network\n",
    "    step = 0\n",
    "    episode = list(range(num_episodes)) # for the progress bar\n",
    "    with tqdm(episode) as pbar:\n",
    "        for i in pbar:\n",
    "            pbar.set_description(\"Training: \"+str(step))\n",
    "            \n",
    "            # initialize the state and its respective action mask\n",
    "            state, info = env.reset()\n",
    "            state = torch.tensor(state, device=device, dtype=torch.float).flatten() \n",
    "            action_mask = torch.tensor(available_actions(state),device=device, dtype=torch.int)\n",
    "\n",
    "            while True:\n",
    "                # generate action and based on that update the env\n",
    "                action_index, action = select_action(state, action_mask)\n",
    "                observation, reward, done, _ , _ = env.step(action)\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                \n",
    "                # if game is not over, generate new action mask for next state\n",
    "                if done:\n",
    "                    next_state = None\n",
    "                    action_mask = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(observation, device=device, dtype=torch.float).flatten()\n",
    "                    action_mask = torch.tensor(available_actions(next_state),device=device, dtype=torch.int16)\n",
    "                \n",
    "                # stack the data into memory\n",
    "                memory.push(state, action_index, next_state, reward, action_mask)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                if (step >= training_episode) :\n",
    "                    optimize_model()\n",
    "                    target_net_state_dict = target_net.state_dict()\n",
    "                    policy_net_state_dict = policy_net.state_dict()\n",
    "                    for key in policy_net_state_dict:\n",
    "                        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                    target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                step += 1\n",
    "\n",
    "    print('Complete')\n",
    "    \n",
    "def select_action(state, available_actions_mask):\n",
    "    \"\"\"\n",
    "    for action selection during training \n",
    "    Inputs: \n",
    "    state: current state for policy to map into a new action\n",
    "    available_actions_mask: mask for avoiding choosing invalid actions\n",
    "    \n",
    "    Returns: action_index, action \n",
    "    \"\"\"\n",
    "    # exponential decay of epsilon\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # optimal\n",
    "            action_index = policy_net(state)\n",
    "            action_index = torch.tensor([action_index[i] if available_actions_mask[i]!=0. else -float(\"Inf\") for i in range(n_actions) ], device = device, dtype = torch.float)\n",
    "            action_index = torch.argmax(action_index).item()\n",
    "    else:\n",
    "        # random\n",
    "        available_actions_mask = [i for i in range(n_actions) if available_actions_mask[i]!=0 ]\n",
    "        action_index = np.random.choice(available_actions_mask)\n",
    "    return torch.tensor(action_index, device = device, dtype=torch.int16), torch.tensor([np.floor(action_index / 3).astype(int), int(action_index % 3)], device = device, dtype = torch.int16)\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"\n",
    "    Updating the networks \n",
    "    \"\"\"\n",
    "    # Sample a batch from the memory, if not enough samples, skip the update\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).view(-1,9)\n",
    "    \n",
    "    # extract the batch elements\n",
    "    state_batch = torch.cat(batch.state).view(BATCH_SIZE,-1)\n",
    "    action_batch = torch.tensor(batch.action, device = device, dtype=torch.int64).view(BATCH_SIZE,-1)\n",
    "    reward_batch = torch.cat(batch.reward).view(BATCH_SIZE,-1)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions which would have been taken according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    _next_state = torch.tensor(np.zeros((BATCH_SIZE, 9)), device = device, dtype = torch.float)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _next_state[non_final_mask] = target_net(non_final_next_states)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if non_final_mask[i]:\n",
    "                _next_state[i] = torch.tensor([_next_state[i][j] if batch.action_mask[i][j]!=0. else -float(\"Inf\") for j in range(n_actions) ], device = device, dtype = torch.float) \n",
    "        next_state_values = _next_state.max(1).values.view(BATCH_SIZE,-1)\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "def available_actions(state):\n",
    "    \"\"\"\n",
    "    Returns a list of all possible actions that can be performed in the given state. \n",
    "\n",
    "    \"\"\"\n",
    "    mask = []\n",
    "    for i in range(9):\n",
    "        if state[i] == 1.0:\n",
    "            mask.append(1)\n",
    "        else:\n",
    "            mask.append(0)\n",
    "\n",
    "#     assert np.shape(available_actions) == (n_actions,), \"available_actions must be a list of length 9.\"\n",
    "#     assert np.count_nonzero(available_actions) == n_actions - env.occupied_fields or 0 , \"error in available action counting.\"\n",
    "\n",
    "    return torch.tensor(mask,device =device, dtype = torch.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation \n",
    "\n",
    "We evaluate the learned polices multiple times against the different opponent policies using the script below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opponent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T03:17:36.528190Z",
     "start_time": "2024-01-08T03:01:21.368525Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(eval_episodes: int):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's policy described by the learned weights by simulating the given number of episodes. \n",
    "    Returns the overall number of wins, draws, looses, and the statistical mean of the episode returns.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    returns = []\n",
    "    wins, draws, looses = 0,0,0\n",
    "\n",
    "    for episode in range(eval_episodes):\n",
    "        \n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent_policy(state)\n",
    "            _, reward, done, _,_ = env.step(action)\n",
    "            if done: \n",
    "                if reward == 1: \n",
    "                    wins = wins + 1\n",
    "                elif reward == 0:\n",
    "                    draws = draws + 1\n",
    "                elif reward == -1:\n",
    "                    looses = looses + 1\n",
    "\n",
    "                returns.append(reward)  \n",
    "          \n",
    "    return wins, draws, looses, np.mean(returns)\n",
    "\n",
    "\n",
    "opponent_policy_dict = opponent_policy_1 # Change to play against different opponent policy.\n",
    "\n",
    "# Policy testing\n",
    "training_episodes = 5000 # Number of training episodes\n",
    "test_episodes = 100 # Number of test episodes\n",
    "test_runs = 5 # Number of test runs\n",
    "\n",
    "for i in range (test_runs):\n",
    "    env = gym.make(\"Sysadmin-ED\") \n",
    "\n",
    "    # create the policy and target nets and make them identical\n",
    "    policy_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    print(policy_net)\n",
    "    print(policy_net.get_parameter_number())\n",
    "    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "    # controls the exponential decay of epsilon\n",
    "    steps_done = 0\n",
    "    \n",
    "    # initialize the replay memory\n",
    "    memory = ReplayMemory(5000)\n",
    "    \n",
    "    # start training  \n",
    "    training_algorithm(training_episodes, env, 3500)\n",
    "    \n",
    "    # Check that number of episodes is not exceeded\n",
    "    if env.get_reset_counter > training_episodes:\n",
    "            raise RuntimeError(f\"Exceeded maximal number of calls of reset function\")\n",
    "    \n",
    "    wins, draws, looses, average_return = evaluate_policy(test_episodes) # evaluate the learned policy\n",
    "    print(f\"Training iteration {i}: Wins: {wins}, Draws: {draws}, Looses: {looses}, Average Return: {average_return}\") # print results of the current test run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opponent 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T03:17:36.528190Z",
     "start_time": "2024-01-08T03:01:21.368525Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(eval_episodes: int):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's policy described by the learned weights by simulating the given number of episodes. \n",
    "    Returns the overall number of wins, draws, looses, and the statistical mean of the episode returns.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    returns = []\n",
    "    wins, draws, looses = 0,0,0\n",
    "\n",
    "    for episode in range(eval_episodes):\n",
    "        \n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent_policy(state)\n",
    "            _, reward, done, _,_ = env.step(action)\n",
    "            if done: \n",
    "                if reward == 1: \n",
    "                    wins = wins + 1\n",
    "                elif reward == 0:\n",
    "                    draws = draws + 1\n",
    "                elif reward == -1:\n",
    "                    looses = looses + 1\n",
    "\n",
    "                returns.append(reward)  \n",
    "          \n",
    "    return wins, draws, looses, np.mean(returns)\n",
    "\n",
    "\n",
    "opponent_policy_dict = opponent_policy_2 # Change to play against different opponent policy.\n",
    "\n",
    "# Policy testing\n",
    "training_episodes = 5000 # Number of training episodes\n",
    "test_episodes = 100 # Number of test episodes\n",
    "test_runs = 5 # Number of test runs\n",
    "\n",
    "for i in range (test_runs):\n",
    "    env = gym.make(\"Sysadmin-ED\") \n",
    "\n",
    "    # create the policy and target nets and make them identical\n",
    "    policy_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    print(policy_net)\n",
    "    print(policy_net.get_parameter_number())\n",
    "    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "    # controls the exponential decay of epsilon\n",
    "    steps_done = 0\n",
    "    \n",
    "    # initialize the replay memory\n",
    "    memory = ReplayMemory(5000)\n",
    "    \n",
    "    # start training  \n",
    "    training_algorithm(training_episodes, env, 3500)\n",
    "    \n",
    "    # Check that number of episodes is not exceeded\n",
    "    if env.get_reset_counter > training_episodes:\n",
    "            raise RuntimeError(f\"Exceeded maximal number of calls of reset function\")\n",
    "    \n",
    "    wins, draws, looses, average_return = evaluate_policy(test_episodes) # evaluate the learned policy\n",
    "    print(f\"Training iteration {i}: Wins: {wins}, Draws: {draws}, Looses: {looses}, Average Return: {average_return}\") # print results of the current test run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opponent 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T03:17:36.528190Z",
     "start_time": "2024-01-08T03:01:21.368525Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(eval_episodes: int):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's policy described by the learned weights by simulating the given number of episodes. \n",
    "    Returns the overall number of wins, draws, looses, and the statistical mean of the episode returns.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    returns = []\n",
    "    wins, draws, looses = 0,0,0\n",
    "\n",
    "    for episode in range(eval_episodes):\n",
    "        \n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent_policy(state)\n",
    "            _, reward, done, _,_ = env.step(action)\n",
    "            if done: \n",
    "                if reward == 1: \n",
    "                    wins = wins + 1\n",
    "                elif reward == 0:\n",
    "                    draws = draws + 1\n",
    "                elif reward == -1:\n",
    "                    looses = looses + 1\n",
    "\n",
    "                returns.append(reward)  \n",
    "          \n",
    "    return wins, draws, looses, np.mean(returns)\n",
    "\n",
    "\n",
    "opponent_policy_dict = opponent_policy_3 # Change to play against different opponent policy.\n",
    "\n",
    "# Policy testing\n",
    "training_episodes = 5000 # Number of training episodes\n",
    "test_episodes = 100 # Number of test episodes\n",
    "test_runs = 5 # Number of test runs\n",
    "\n",
    "for i in range (test_runs):\n",
    "    env = gym.make(\"Sysadmin-ED\") \n",
    "\n",
    "    # create the policy and target nets and make them identical\n",
    "    policy_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    print(policy_net)\n",
    "    print(policy_net.get_parameter_number())\n",
    "    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "    # controls the exponential decay of epsilon\n",
    "    steps_done = 0\n",
    "    \n",
    "    # initialize the replay memory\n",
    "    memory = ReplayMemory(5000)\n",
    "    \n",
    "    # start training  \n",
    "    training_algorithm(training_episodes, env, 3500)\n",
    "    \n",
    "    # Check that number of episodes is not exceeded\n",
    "    if env.get_reset_counter > training_episodes:\n",
    "            raise RuntimeError(f\"Exceeded maximal number of calls of reset function\")\n",
    "    \n",
    "    wins, draws, looses, average_return = evaluate_policy(test_episodes) # evaluate the learned policy\n",
    "    print(f\"Training iteration {i}: Wins: {wins}, Draws: {draws}, Looses: {looses}, Average Return: {average_return}\") # print results of the current test run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opponent 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-08T03:49:33.883Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(eval_episodes: int):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's policy described by the learned weights by simulating the given number of episodes. \n",
    "    Returns the overall number of wins, draws, looses, and the statistical mean of the episode returns.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    returns = []\n",
    "    wins, draws, looses = 0,0,0\n",
    "\n",
    "    for episode in range(eval_episodes):\n",
    "        \n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent_policy(state)\n",
    "            _, reward, done, _,_ = env.step(action)\n",
    "            if done: \n",
    "                if reward == 1: \n",
    "                    wins = wins + 1\n",
    "                elif reward == 0:\n",
    "                    draws = draws + 1\n",
    "                elif reward == -1:\n",
    "                    looses = looses + 1\n",
    "\n",
    "                returns.append(reward)  \n",
    "          \n",
    "    return wins, draws, looses, np.mean(returns)\n",
    "\n",
    "\n",
    "opponent_policy_dict = opponent_policy_4 # Change to play against different opponent policy.\n",
    "\n",
    "# Policy testing\n",
    "training_episodes = 5000 # Number of training episodes\n",
    "test_episodes = 100 # Number of test episodes\n",
    "test_runs = 5 # Number of test runs\n",
    "\n",
    "for i in range (test_runs):\n",
    "    env = gym.make(\"Sysadmin-ED\") \n",
    "\n",
    "    # create the policy and target nets and make them identical\n",
    "    policy_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "    print(policy_net)\n",
    "    print(policy_net.get_parameter_number())\n",
    "    # controls the exponential decay of epsilon\n",
    "    steps_done = 0\n",
    "    \n",
    "    # initialize the replay memory\n",
    "    memory = ReplayMemory(5000)\n",
    "    \n",
    "    # start training  \n",
    "    training_algorithm(training_episodes, env, 3500)\n",
    "    \n",
    "    # Check that number of episodes is not exceeded\n",
    "    if env.get_reset_counter > training_episodes:\n",
    "            raise RuntimeError(f\"Exceeded maximal number of calls of reset function\")\n",
    "    \n",
    "    wins, draws, looses, average_return = evaluate_policy(test_episodes) # evaluate the learned policy\n",
    "    print(f\"Training iteration {i}: Wins: {wins}, Draws: {draws}, Looses: {looses}, Average Return: {average_return}\") # print results of the current test run\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
